# Test scenarios for azure-ai-inference-dotnet skill evaluation
# Each scenario tests a specific usage pattern against acceptance criteria

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  # Basic Chat Completion
  - name: basic_chat_completion
    prompt: |
      Create a basic chat completion example using Azure.AI.Inference SDK.
      Include proper authentication with DefaultAzureCredential,
      ChatCompletionsClient creation, and extracting the response content.
    expected_patterns:
      - "DefaultAzureCredential"
      - "ChatCompletionsClient"
      - "ChatCompletionsOptions"
      - "ChatRequestSystemMessage"
      - "ChatRequestUserMessage"
      - "client\\.Complete"
      - "Choices\\[0\\]\\.Message\\.Content"
    forbidden_patterns:
      - "AzureOpenAIClient"
      - "new SystemMessage"
      - "new UserMessage"
      - "response\\.Content"
    tags:
      - basic
      - authentication
      - chat-completions
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;

      var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
      var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

      var requestOptions = new ChatCompletionsOptions()
      {
          Messages =
          {
              new ChatRequestSystemMessage("You are a helpful assistant."),
              new ChatRequestUserMessage("What is the capital of France?"),
          },
      };

      Response<ChatCompletions> response = client.Complete(requestOptions);
      Console.WriteLine($"Assistant: {response.Value.Choices[0].Message.Content}");

  # Async Chat Completion
  - name: async_chat_completion
    prompt: |
      Create an async chat completion example using CompleteAsync method.
      Use proper async/await patterns and task-based concurrency.
    expected_patterns:
      - "async"
      - "await client\\.CompleteAsync"
      - "Task"
      - "ChatRequestSystemMessage"
      - "ChatRequestUserMessage"
    forbidden_patterns:
      - "client\\.Complete\\("
      - "\\.Result"
    tags:
      - async
      - chat-completions
      - advanced
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Threading.Tasks;

      async Task Main()
      {
          var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
          var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

          var requestOptions = new ChatCompletionsOptions()
          {
              Messages =
              {
                  new ChatRequestSystemMessage("You are helpful."),
                  new ChatRequestUserMessage("Hello!"),
              },
          };

          Response<ChatCompletions> response = await client.CompleteAsync(requestOptions);
          Console.WriteLine(response.Value.Choices[0].Message.Content);
      }

      await Main();

  # Streaming Chat Completion
  - name: streaming_chat_completion
    prompt: |
      Create a streaming chat completion example that processes
      chat updates in real-time. Use CompleteStreamingAsync and iterate
      through ContentUpdate values.
    expected_patterns:
      - "CompleteStreamingAsync"
      - "StreamingResponse<StreamingChatCompletionsUpdate>"
      - "await foreach"
      - "ContentUpdate"
      - "StringBuilder"
    forbidden_patterns:
      - "CompleteAsync"
      - "stream=true"
      - "client\\.Complete\\("
    tags:
      - streaming
      - chat-completions
      - advanced
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Text;
      using System.Threading.Tasks;

      async Task Main()
      {
          var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
          var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

          var requestOptions = new ChatCompletionsOptions()
          {
              Messages =
              {
                  new ChatRequestSystemMessage("You are a helpful assistant."),
                  new ChatRequestUserMessage("Write a short poem about Azure."),
              },
          };

          StreamingResponse<StreamingChatCompletionsUpdate> response = 
              await client.CompleteStreamingAsync(requestOptions);

          StringBuilder contentBuilder = new();
          await foreach (StreamingChatCompletionsUpdate chatUpdate in response)
          {
              if (!string.IsNullOrEmpty(chatUpdate.ContentUpdate))
              {
                  contentBuilder.Append(chatUpdate.ContentUpdate);
                  Console.Write(chatUpdate.ContentUpdate);
              }
          }
          Console.WriteLine();
      }

      await Main();

  # Multi-modal Chat (Images)
  - name: multimodal_chat_with_images
    prompt: |
      Create a multi-modal chat example that includes images.
      Use ChatMessageTextContentItem and ChatMessageImageContentItem
      to construct a message with both text and image content.
    expected_patterns:
      - "ChatMessageTextContentItem"
      - "ChatMessageImageContentItem"
      - "new Uri"
      - "ChatRequestUserMessage"
    forbidden_patterns:
      - "new ChatMessage("
      - "ImageUrl"
    tags:
      - multimodal
      - images
      - chat-completions
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;

      var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
      var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

      var requestOptions = new ChatCompletionsOptions()
      {
          Messages =
          {
              new ChatRequestSystemMessage("You are a helpful assistant that describes images."),
              new ChatRequestUserMessage(
                  new ChatMessageTextContentItem("What's in this image?"),
                  new ChatMessageImageContentItem(new Uri("https://example.com/image.jpg"))
              ),
          },
      };

      Response<ChatCompletions> response = client.Complete(requestOptions);
      Console.WriteLine(response.Value.Choices[0].Message.Content);

  # Function Calling / Tool Use
  - name: function_tool_calling
    prompt: |
      Create a function calling example with ChatCompletionsFunctionToolDefinition.
      Define a get_weather function, handle tool calls, and execute the function.
      Include proper ToolCall handling and ChatRequestToolMessage for results.
    expected_patterns:
      - "ChatCompletionsFunctionToolDefinition"
      - "FunctionDefinition"
      - "get_weather"
      - "ToolCalls"
      - "ChatCompletionsFunctionToolCall"
      - "ChatRequestToolMessage"
    forbidden_patterns:
      - "new FunctionToolDefinition("
      - "tools="
    tags:
      - function-calling
      - tools
      - advanced
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Collections.Generic;

      var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
      var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

      var getWeatherTool = new ChatCompletionsFunctionToolDefinition(
          new FunctionDefinition("get_weather")
          {
              Description = "Get the weather in a location",
              Parameters = BinaryData.FromString("""
              {
                  "type": "object",
                  "properties": {
                      "location": { "type": "string", "description": "The city and state" }
                  },
                  "required": ["location"]
              }
              """)
          });

      var requestOptions = new ChatCompletionsOptions()
      {
          Messages =
          {
              new ChatRequestUserMessage("What's the weather in Seattle?"),
          },
          Tools = { getWeatherTool }
      };

      Response<ChatCompletions> response = client.Complete(requestOptions);
      
      var assistantMessage = response.Value.Choices[0].Message;
      if (assistantMessage.ToolCalls?.Count > 0)
      {
          foreach (var toolCall in assistantMessage.ToolCalls)
          {
              if (toolCall is ChatCompletionsFunctionToolCall functionCall)
              {
                  string result = "72Â°F and sunny";
                  requestOptions.Messages.Add(new ChatRequestAssistantMessage(assistantMessage));
                  requestOptions.Messages.Add(new ChatRequestToolMessage(result, functionCall.Id));
              }
          }
          
          response = client.Complete(requestOptions);
          Console.WriteLine(response.Value.Choices[0].Message.Content);
      }

  # Text Embeddings
  - name: text_embeddings
    prompt: |
      Create a text embeddings example using EmbeddingsClient.
      Generate embeddings for multiple text inputs and extract the embedding vectors.
    expected_patterns:
      - "EmbeddingsClient"
      - "EmbeddingsOptions"
      - "client\\.Embed"
      - "ToObjectFromJson<List<float>>"
      - "foreach"
      - "item\\.Embedding"
    forbidden_patterns:
      - "embedding as float"
      - "item\\.Embedding\\.Value"
      - "ChatCompletionsClient"
    tags:
      - embeddings
      - vectors
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Collections.Generic;

      var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_EMBEDDINGS_ENDPOINT"));
      var credential = new AzureKeyCredential(Environment.GetEnvironmentVariable("AZURE_AI_EMBEDDINGS_KEY"));
      var client = new EmbeddingsClient(endpoint, credential, new AzureAIInferenceClientOptions());

      var input = new List<string> { "King", "Queen", "Jack", "Page" };
      var requestOptions = new EmbeddingsOptions(input);

      Response<EmbeddingsResult> response = client.Embed(requestOptions);

      foreach (EmbeddingItem item in response.Value.Data)
      {
          List<float> embedding = item.Embedding.ToObjectFromJson<List<float>>();
          Console.WriteLine($"Index: {item.Index}, Embedding length: {embedding.Count}");
      }

  # Error Handling with RequestFailedException
  - name: error_handling_with_retry
    prompt: |
      Create an error handling example that catches RequestFailedException,
      checks the Status code, and implements retry logic for rate limiting (429).
    expected_patterns:
      - "RequestFailedException"
      - "catch"
      - "ex\\.Status"
      - "429"
      - "try"
    forbidden_patterns:
      - "catch \\(Exception"
      - "Empty catch"
    tags:
      - error-handling
      - resilience
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Threading.Tasks;

      async Task Main()
      {
          var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
          var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

          var requestOptions = new ChatCompletionsOptions()
          {
              Messages =
              {
                  new ChatRequestUserMessage("Hello!"),
              },
          };

          try
          {
              Response<ChatCompletions> response = await client.CompleteAsync(requestOptions);
              Console.WriteLine(response.Value.Choices[0].Message.Content);
          }
          catch (RequestFailedException ex)
          {
              Console.WriteLine($"Status: {ex.Status}");
              Console.WriteLine($"Error: {ex.Message}");
              
              switch (ex.Status)
              {
                  case 429:
                      Console.WriteLine("Rate limited - implement backoff");
                      break;
                  case 401:
                      Console.WriteLine("Unauthorized - check credentials");
                      break;
              }
          }
      }

      await Main();

  # Model Information Retrieval
  - name: get_model_info
    prompt: |
      Create an example that retrieves model information using GetModelInfoAsync.
      Display the model name, type, and provider information.
    expected_patterns:
      - "GetModelInfoAsync"
      - "ModelInfo"
      - "Value\\.ModelName"
      - "Value\\.ModelType"
      - "Value\\.ModelProviderName"
    forbidden_patterns:
      - "GetModelInfo\\(\\)"
      - "\\.ModelName"
    tags:
      - model-info
      - basic
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Threading.Tasks;

      async Task Main()
      {
          var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
          var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

          Response<ModelInfo> modelInfo = await client.GetModelInfoAsync();
          
          Console.WriteLine($"Model name: {modelInfo.Value.ModelName}");
          Console.WriteLine($"Model type: {modelInfo.Value.ModelType}");
           Console.WriteLine($"Model provider: {modelInfo.Value.ModelProviderName}");
       }

       await Main();

  # Image Embeddings
  - name: image_embeddings
    prompt: |
      Create an image embeddings example using ImageEmbeddingsClient.
      Generate embeddings for images from URLs using ImageEmbeddingsOptions.
    expected_patterns:
      - "ImageEmbeddingsClient"
      - "ImageEmbeddingsOptions"
      - "ImageEmbeddingInput"
      - "new Uri"
      - "EmbedAsync"
    forbidden_patterns:
      - "EmbeddingsClient with ImageEmbeddingInput"
      - "new EmbeddingsOptions("
    tags:
      - embeddings
      - images
      - advanced
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;
      using System.Threading.Tasks;

      async Task Main()
      {
          var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_EMBEDDINGS_ENDPOINT"));
          var credential = new AzureKeyCredential(Environment.GetEnvironmentVariable("AZURE_AI_EMBEDDINGS_KEY"));
          var client = new ImageEmbeddingsClient(endpoint, credential, new AzureAIInferenceClientOptions());

          var requestOptions = new ImageEmbeddingsOptions()
          {
              Input = 
              {
                  new ImageEmbeddingInput(new Uri("https://example.com/image.jpg")),
              }
          };

          Response<EmbeddingsResult> response = await client.EmbedAsync(requestOptions);
          Console.WriteLine($"Got {response.Value.Data.Count} embeddings");
      }

      await Main();

  # Temperature and Additional Parameters
  - name: temperature_and_parameters
    prompt: |
      Create a chat completion example that demonstrates Temperature for controlling
      output creativity and MaxTokens to limit response length.
    expected_patterns:
      - "Temperature"
      - "MaxTokens"
      - "0\\.[0-9]"
      - "ChatCompletionsOptions"
    forbidden_patterns:
      - "temperature as string"
      - "max_tokens"
    tags:
      - parameters
      - configuration
    mock_response: |
      using Azure;
      using Azure.AI.Inference;
      using Azure.Identity;
      using System;

      var endpoint = new Uri(Environment.GetEnvironmentVariable("AZURE_AI_CHAT_ENDPOINT"));
      var client = new ChatCompletionsClient(endpoint, new DefaultAzureCredential());

      var requestOptions = new ChatCompletionsOptions()
      {
          Messages =
          {
              new ChatRequestSystemMessage("You are a helpful assistant."),
              new ChatRequestUserMessage("Write a haiku about coding."),
          },
          Temperature = 0.7f,
          MaxTokens = 100,
      };

      Response<ChatCompletions> response = client.Complete(requestOptions);
      Console.WriteLine(response.Value.Choices[0].Message.Content);
